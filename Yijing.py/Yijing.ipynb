{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip3 install seaborn\n",
    "#%pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, save, load\n",
    "from torch.optim import Adam\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from Sequences import Sequences\n",
    "from ValueSequencer import HexagramValueSequencer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "  return 1 / (1 + np.exp(-z)) # aka 1 / 1 + e^-x\n",
    "\n",
    "def test_sigmoid():\n",
    "\t# ðŸ‘‡ OUR \"ARTIFICIAL\" NEURON...\n",
    "\tinput = 0.2 # fmr. called x\n",
    "\tweight = 1 # fmr. called m\n",
    "\tbias = 0 # # fmr. called b\n",
    "\td_x = weight*input + bias\n",
    "\ta_x = sigmoid\n",
    "\toutput = a_x(d_x)\n",
    "\n",
    "\tprint(f\"Input: {input}\")\n",
    "\tprint(f\"Weight: {weight}\")\n",
    "\tprint(f\"Bias: {bias}\")\n",
    "\tprint(f\"Output: {output}\")\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GenerateSequence1(thvs, inputs):\n",
    "\thvs1 = HexagramValueSequencer()\n",
    "\thvs1._currentSequence = 0\n",
    "\thvs1.First()\n",
    "\tp1 = 0\n",
    "\tfor p in range(64):\n",
    "\t\tfor s in range(64):\n",
    "\t\t\thvs2 = hvs1.copy()\n",
    "\n",
    "\t\t\tfor l in range(6):\n",
    "\t\t\t\tif ((hvs1.Value() < 32) and (((s & (1 << l)) >> l) == 0)) or ((hvs1.Value() >= 32) and (((s & (1 << l)) >> l) == 1)): # p -> hvs1.Value()\n",
    "\t\t\t\t\thvs2.Trigram(l // 3).Line(l % 3).Next(False)\n",
    "\n",
    "\t\t\t#if (p * 64 + s < 64) or (p * 64 + s > 4031):\n",
    "\t\t\t#\tprint(hvs2.DescribeCast(True))\n",
    "\n",
    "\t\t\t#hvs2.Move()\n",
    "\t\t\tfor l in range(6):\n",
    "\t\t\t\tv = hvs2.Trigram(l // 3).Line(l % 3).Value()\n",
    "\t\t\t\tv0 = 0\n",
    "\t\t\t\tv1 = 0\n",
    "\t\t\t\tv2 = 0\n",
    "\t\t\t\tif v == 1:\n",
    "\t\t\t\t\tv0 = 1\n",
    "\t\t\t\t\tv2 = 1\n",
    "\t\t\t\tif v == 2:\n",
    "\t\t\t\t\tv1 = 1\n",
    "\t\t\t\tif v == 3:\n",
    "\t\t\t\t\tv0 = 1\n",
    "\t\t\t\t\tv1 = 1\n",
    "\t\t\t\t\tv2 = 1\n",
    "\t\t\t\tthvs[p * 64 + s, l, 0] = v0\n",
    "\t\t\t\tthvs[p * 64 + s, l, 1] = v1 \n",
    "\t\t\t\tthvs[p * 64 + s, l, 2] = v2 \n",
    "\n",
    "\t\tc = 0\n",
    "\t\tfor l in range(5, -1, -1):\n",
    "\t\t\tv = hvs1.Trigram(l // 3).Line(l % 3).Value()\n",
    "\t\t\tv0 = 0\n",
    "\t\t\tv1 = 0\n",
    "\t\t\tv2 = 0\n",
    "\t\t\tif v == 1:\n",
    "\t\t\t\tv0 = 1\n",
    "\t\t\t\tv2 = 1\n",
    "\t\t\tif v == 2:\n",
    "\t\t\t\tv1 = 1\n",
    "\t\t\tif v == 3:\n",
    "\t\t\t\tv0 = 1\n",
    "\t\t\t\tv1 = 1\n",
    "\t\t\t\tv2 = 1\n",
    "\t\t\tinputs[p, c] = v0# / 10\n",
    "\t\t\tinputs[p, c] = v1# / 10\n",
    "\t\t\tinputs[p, c] = v2# / 10\n",
    "\t\t\tc += 1 \n",
    "\n",
    "\t\thvs1.Next()\n",
    "\n",
    "\t#print()\t\n",
    "\t#print(thvs)\n",
    "\n",
    "\t#print(inputs)\n",
    "\t#print()\t\n",
    "\n",
    "\t#print()\t\n",
    "\t#hexagramArray.sort()\n",
    "\n",
    "class YijingNN1(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(YijingNN1, self).__init__()\n",
    "\t\tself.input_layer = nn.Linear(6, 64)\n",
    "\t\t\n",
    "\t\tself.layer_1 = nn.Linear(64, 1)\n",
    "\n",
    "\t\t#self.layer_1 = nn.Linear(64, 64)\n",
    "\t\t#self.layer_2 = nn.Linear(64, 1)\n",
    "\t\t\n",
    "\t\t# Sigmoid function for output\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\t\t# Initialize weights to 0\n",
    "\t\t#self.initialize_weights()\n",
    "\n",
    "\tdef initialize_weights(self):\n",
    "\t\tnn.init.constant_(self.input_layer.weight, 0)\n",
    "\t\tnn.init.constant_(self.input_layer.bias, 0.5)\n",
    "\t\tnn.init.constant_(self.layer_1.weight, 0)\n",
    "\t\tnn.init.constant_(self.layer_1.bias, 0.5)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = torch.relu(self.input_layer(x))\n",
    "\t\t\n",
    "\t\tx = self.layer_1(x)\n",
    "\t\t\n",
    "\t\t#x = torch.relu(self.layer_1(x))\n",
    "\t\t#x = self.layer_2(x)\n",
    "\t\t\n",
    "\t\tx = self.sigmoid(x)\n",
    "\t\treturn x\n",
    "\n",
    "def train_yijing_model1():\n",
    "\tthvs = torch.zeros(4096, 6, 3, dtype=torch.int, requires_grad=False) # 2 ^ 6 = 64, 4 ^ 6 = 4096, 8 ^ 6 = 262,144\n",
    "\tinputs = torch.zeros(64, 6, dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "\tlabels = torch.tensor([\n",
    "\t\t[0.00],\t[0.01],\t[0.02],\t[0.03],\t[0.04],\t[0.05],\t[0.06],\t[0.07],\t[0.08],\t[0.09],\n",
    "\t\t[0.10],\t[0.11],\t[0.12],\t[0.13],\t[0.14],\t[0.15],\t[0.16],\t[0.17],\t[0.18],\t[0.19],\n",
    "\t\t[0.20],\t[0.21],\t[0.22],\t[0.23],\t[0.24],\t[0.25],\t[0.26],\t[0.27],\t[0.28],\t[0.29],\n",
    "\t\t[0.30],\t[0.31],\t[0.32],\t[0.33],\t[0.34],\t[0.35],\t[0.36],\t[0.37],\t[0.38],\t[0.39],\n",
    "\t\t[0.40],\t[0.41],\t[0.42],\t[0.43],\t[0.44],\t[0.45],\t[0.46],\t[0.47],\t[0.48],\t[0.49],\n",
    "\t\t[0.50],\t[0.51],\t[0.52],\t[0.53],\t[0.54],\t[0.55],\t[0.56],\t[0.57],\t[0.58],\t[0.59],\n",
    "\t\t[0.60],\t[0.61],\t[0.62],\t[0.63],\n",
    "\t], dtype=torch.float32, requires_grad=False)\n",
    "\t\n",
    "\tGenerateSequence1(thvs, inputs)\n",
    "\n",
    "\tmodel = YijingNN1()\n",
    "\n",
    "\t# Define a loss function and an optimizer\n",
    "\tcriterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
    "\toptimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent optimizer  0.001\n",
    "\n",
    "\t# Training loop\n",
    "\tnum_epochs = 50000 #100000\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\t# Zero the gradients\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\t# Forward pass: compute the model output\n",
    "\t\toutputs = model(inputs)\n",
    "\t\t# Compute the loss\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\t# Backward pass: compute gradients\n",
    "\t\tloss.backward()\n",
    "\t\t# Update weights\n",
    "\t\toptimizer.step()\n",
    "\t\t# Print loss every 100 epochs\n",
    "\t\tif (epoch+1) % 10000 == 0:\n",
    "\t\t\tprint(f'Epoch [{epoch+1}/{num_epochs}], Cost: {loss.item():.4f}')\n",
    "\n",
    "\tprint()\n",
    "\n",
    "\twith open('yijing_model1_state.pt', 'wb') as f: \n",
    "\t\tsave(model.state_dict(), f) \n",
    "\n",
    "def test_yijing_model1():\n",
    "\n",
    "\tmodel = YijingNN1()\n",
    "\n",
    "\twith open('yijing_model1_state.pt', 'rb') as f: \n",
    "\t\tmodel.load_state_dict(load(f, weights_only=False))  \n",
    "\n",
    "\th1 = torch.tensor([[0, 0, 0, 0, 0, 0]], dtype=torch.float32)\n",
    "\th2 = torch.tensor([[0, 0, 0, 0, 0, 1]], dtype=torch.float32)\n",
    "\th3 = torch.tensor([[1, 0, 0, 0, 0, 0]], dtype=torch.float32)\n",
    "\th4 = torch.tensor([[1, 1, 1, 1, 1, 0]], dtype=torch.float32)\n",
    "\th5 = torch.tensor([[1, 1, 1, 1, 1, 1]], dtype=torch.float32)\n",
    "\n",
    "\tprediction1 = model(h1)\n",
    "\tprediction2 = model(h2)\n",
    "\tprediction3 = model(h3)\n",
    "\tprediction4 = model(h4)\n",
    "\tprediction5 = model(h5)\n",
    "\tprint(f'Hexagram  0: {h1} Prediction: {prediction1.item() * 100:.0f}')\n",
    "\tprint(f'Hexagram  1: {h2} Prediction: {prediction2.item() * 100:.0f}')\n",
    "\tprint(f'Hexagram 32: {h3} Prediction: {prediction3.item() * 100:.0f}')\n",
    "\tprint(f'Hexagram 62: {h4} Prediction: {prediction4.item() * 100:.0f}')\n",
    "\tprint(f'Hexagram 63: {h5} Prediction: {prediction5.item() * 100:.0f}')\n",
    "\tprint()\n",
    "\n",
    "#train_yijing_model1()\n",
    "test_yijing_model1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateSequence2(inputs, labels):\n",
    "\thvs1 = HexagramValueSequencer()\n",
    "\thvs1._currentSequence = 0\n",
    "\thvs1.First()\n",
    "\tfor p in range(64):\n",
    "\t\tfor s in range(64):\n",
    "\t\t\thvs2 = hvs1.copy()\n",
    "\t\t\tfor l in range(6):\n",
    "\t\t\t\t#if ((hvs1.Value() < 32) and (((s & (1 << l)) >> l) == 0)) or ((hvs1.Value() >= 32) and (((s & (1 << l)) >> l) == 1)):\n",
    "\t\t\t\tif ((s & (1 << l)) >> l) == 1:\n",
    "\t\t\t\t\thvs2.Trigram(l // 3).Line(l % 3).Next(False)\n",
    "\n",
    "\t\t\tc = 0\n",
    "\t\t\tfor l in range(5, -1, -1):\n",
    "\t\t\t\tinputs[p * 64 + s, c] = hvs2.Trigram(l // 3).Line(l % 3).Value()\n",
    "\t\t\t\tc += 1 \n",
    "\n",
    "\t\t\ts1 = hvs2.DescribeCast(True).split(\" \")\n",
    "\t\t\t#f = np.float64(float(s1[0])) / 100\n",
    "\t\t\tf = p / 100 + s / 10000\n",
    "\t\t\tlabels[p * 64 + s] = f\n",
    "\n",
    "\t\t\t#if (p * 64 + s < 64) or (p * 64 + s > 4031):\n",
    "\t\t\t#\tprint(f'{hvs2.DescribeCast(True)}, {f:.4f}')\n",
    "\n",
    "\t\thvs1.Next()\n",
    "\n",
    "\t#print()\t\n",
    "\t#print(thvs)\n",
    "\n",
    "\t#print(inputs)\n",
    "\t#print()\t\n",
    "\n",
    "\t#print()\t\n",
    "\t#hexagramArray.sort()\n",
    "\n",
    "class YijingNN2(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(YijingNN2, self).__init__()\n",
    "\t\tself.input_layer = nn.Linear(6, 4096, dtype=torch.float32)\n",
    "\t\t\n",
    "\t\tself.layer_1 = nn.Linear(4096, 1, dtype=torch.float32)\n",
    "\n",
    "\t\t#self.layer_1 = nn.Linear(64, 64)\n",
    "\t\t#self.layer_2 = nn.Linear(64, 1)\n",
    "\t\t\n",
    "\t\t# Sigmoid function for output\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\t\t# Initialize weights to 0\n",
    "\t\t#self._initialize_weights()\n",
    "\n",
    "\tdef x_initialize_weights(self):\n",
    "\t\tnn.init.constant_(self.input_layer.weight, 0)\n",
    "\t\tnn.init.constant_(self.input_layer.bias, 0)\n",
    "\t\tnn.init.constant_(self.layer_1.weight, 0)\n",
    "\t\tnn.init.constant_(self.layer_1.bias, 0)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = torch.relu(self.input_layer(x))\n",
    "\t\t\n",
    "\t\tx = self.layer_1(x)\n",
    "\t\t\n",
    "\t\t#x = torch.relu(self.layer_1(x))\n",
    "\t\t#x = self.layer_2(x)\n",
    "\t\t\n",
    "\t\tx = self.sigmoid(x)\n",
    "\t\treturn x\n",
    "\n",
    "def train_yijing_model2():\n",
    "\tinputs = torch.zeros(4096, 6, dtype=torch.float32, requires_grad=False).to('cpu')\n",
    "\tlabels = torch.zeros(4096, 1, dtype=torch.float32, requires_grad=False).to('cpu')\n",
    "\n",
    "\tGenerateSequence2(inputs, labels)\n",
    "\tprint(inputs)\n",
    "\tprint(labels)\n",
    "\n",
    "\tmodel = YijingNN2().to('cpu')\n",
    "\n",
    "\t# Define a loss function and an optimizer\n",
    "\tcriterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
    "\toptimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent optimizer  0.001\n",
    "\n",
    "\t# Training loop\n",
    "\tnum_epochs = 10_000 #50_000\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\t# Zero the gradients\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\t# Forward pass: compute the model output\n",
    "\t\toutputs = model(inputs)\n",
    "\t\t# Compute the loss\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\t# Backward pass: compute gradients\n",
    "\t\tloss.backward()\n",
    "\t\t# Update weights\n",
    "\t\toptimizer.step()\n",
    "\t\t# Print loss every 100 epochs\n",
    "\t\tif (epoch+1) % 1_000 == 0:\n",
    "\t\t\tprint(f'Epoch [{epoch+1}/{num_epochs}], Cost: {loss.item():.4f}')\n",
    "\n",
    "\tprint()\n",
    "\n",
    "\twith open('yijing_model2_state.pt', 'wb') as f: \n",
    "\t\tsave(model.state_dict(), f) \n",
    "\n",
    "def test_yijing_model2():\n",
    "\n",
    "\tmodel = YijingNN2()\n",
    "\n",
    "\twith open('yijing_model2_state.pt', 'rb') as f: \n",
    "\t\tmodel.load_state_dict(load(f, weights_only=False))  \n",
    "\n",
    "\th1 = torch.tensor([[2, 2, 2, 2, 2, 2]], dtype=torch.float32)\n",
    "\th2 = torch.tensor([[2, 2, 2, 0, 0, 3]], dtype=torch.float32)\n",
    "\th3 = torch.tensor([[3, 3, 3, 2, 2, 2]], dtype=torch.float32)\n",
    "\th4 = torch.tensor([[1, 1, 1, 3, 3, 0]], dtype=torch.float32)\n",
    "\th5 = torch.tensor([[1, 1, 1, 1, 1, 1]], dtype=torch.float32)\n",
    "\n",
    "\tprediction1 = model(h1)\n",
    "\tprediction2 = model(h2)\n",
    "\tprediction3 = model(h3)\n",
    "\tprediction4 = model(h4)\n",
    "\tprediction5 = model(h5)\n",
    "\tprint(f'Hexagram 0: {h1} Prediction: {prediction1.item():.4f}')\n",
    "\tprint(f'Hexagram 1.123: {h2} Prediction: {prediction2.item():.4f}')\n",
    "\tprint(f'Hexagram 57.456: {h3} Prediction: {prediction3.item():.4f}')\n",
    "\tprint(f'Hexagram 62.123: {h4} Prediction: {prediction4.item():.4f}')\n",
    "\tprint(f'Hexagram 63: {h5} Prediction: {prediction5.item():.4f}')\n",
    "\tprint()\n",
    "\n",
    "#train_yijing_model2()\n",
    "test_yijing_model2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class EegNN(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(EegNN, self).__init__()\n",
    "\t\tself.input_layer = nn.Linear(20, 100)\n",
    "\t\t#self.layer_1 = nn.Linear(100, 1)\n",
    "\t\tself.layer_1 = nn.Linear(100, 20)\n",
    "\t\tself.layer_2 = nn.Linear(20, 10)\n",
    "\t\tself.layer_3 = nn.Linear(10, 1)\n",
    "\t\t# Sigmoid function for output\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = torch.relu(self.input_layer(x))\n",
    "\t\t#x = self.layer_1(x)\n",
    "\t\tx = torch.relu(self.layer_1(x))\n",
    "\t\tx = torch.relu(self.layer_2(x))\n",
    "\t\tx = self.layer_3(x)\n",
    "\t\tx = self.sigmoid(x)\n",
    "\t\treturn x\n",
    "\n",
    "\t\t\n",
    "class EegNN(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(EegNN, self).__init__()\n",
    "\t\tself.model = nn.Sequential(\n",
    "\t\t\tnn.Linear(20, 100),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(100, 20),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(20, 10),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(10, 1),\n",
    "\t\t\tnn.Sigmoid()\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.model(x)\n",
    "\n",
    "class EegNN(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(EegNN, self).__init__()\n",
    "\t\tself.input_layer = nn.Linear(20, 100, dtype=torch.float32)\n",
    "\t\t\n",
    "\t\t#self.layer_1 = nn.Linear(100, 1, dtype=torch.float32)\n",
    "\n",
    "\t\tself.layer_1 = nn.Linear(100, 10)\n",
    "\t\tself.layer_2 = nn.Linear(10, 1)\n",
    "\t\t\n",
    "\t\t# Sigmoid function for output\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\t\t# Initialize weights to 0\n",
    "\t\t#self._initialize_weights()\n",
    "\n",
    "\tdef x_initialize_weights(self):\n",
    "\t\tnn.init.constant_(self.input_layer.weight, 0)\n",
    "\t\tnn.init.constant_(self.input_layer.bias, 0)\n",
    "\t\tnn.init.constant_(self.layer_1.weight, 0)\n",
    "\t\tnn.init.constant_(self.layer_1.bias, 0)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = torch.relu(self.input_layer(x))\n",
    "\t\t\n",
    "\t\t#x = self.layer_1(x)\n",
    "\t\t\n",
    "\t\tx = torch.relu(self.layer_1(x))\n",
    "\t\tx = self.layer_2(x)\n",
    "\t\t\n",
    "\t\tx = self.sigmoid(x)\n",
    "\t\treturn x\n",
    "\"\"\"\n",
    "\n",
    "class EegNN(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(EegNN, self).__init__()\n",
    "\t\tself.model = nn.Sequential(\n",
    "\t\t\tnn.Linear(20, 100),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(100, 1),\n",
    "\t\t\tnn.Sigmoid()\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.model(x)\n",
    "\n",
    "def GenerateData(inputs, labels):\n",
    "\n",
    "\tDelta_TP9_i = 0\n",
    "\tDelta_AF7_i = 1\n",
    "\tDelta_AF8_i = 2\n",
    "\tDelta_TP10_i = 3\n",
    "\tTheta_TP9_i = 4\n",
    "\tTheta_AF7_i = 5\n",
    "\tTheta_AF8_i = 6\n",
    "\tTheta_TP10_i = 7\n",
    "\tAlpha_TP9_i = 8\n",
    "\tAlpha_AF7_i = 9\n",
    "\tAlpha_AF8_i = 10\n",
    "\tAlpha_TP10_i = 11\n",
    "\tBeta_TP9_i = 12\n",
    "\tBeta_AF7_i = 13\n",
    "\tBeta_AF8_i = 14\n",
    "\tBeta_TP10_i = 15\n",
    "\tGamma_TP9_i = 16\n",
    "\tGamma_AF7_i = 17\n",
    "\tGamma_AF8_i = 18\n",
    "\tGamma_TP10_i = 19\n",
    "\n",
    "\tDelta_AF7_min = -0.5\n",
    "\tDelta_AF8_min = -0.5\n",
    "\tDelta_AF7_v = 0.0\n",
    "\tDelta_AF8_v = 0.0\n",
    "\n",
    "\tDelta_TP9_min = -0.1\n",
    "\tDelta_TP10_min = -0.1\n",
    "\tDelta_TP9_v = 0.2\n",
    "\tDelta_TP10_v = 0.2\n",
    "\t\n",
    "\tTheta_AF7_min = -0.5\n",
    "\tTheta_AF8_min = -0.5\n",
    "\tTheta_AF7_v = 0.0\n",
    "\tTheta_AF8_v = 0.0\n",
    "\t\n",
    "\tTheta_TP9_min = -0.1\n",
    "\tTheta_TP10_min = -0.1\n",
    "\tTheta_TP9_v = 0.2\n",
    "\tTheta_TP10_v = 0.2\n",
    "\t\n",
    "\tAlpha_AF7_min = 0.0\n",
    "\tAlpha_AF8_min = 0.0\n",
    "\tAlpha_AF7_v = 0.5\n",
    "\tAlpha_AF8_v = 0.5\n",
    "\t\n",
    "\tAlpha_TP9_min = 0.4\n",
    "\tAlpha_TP10_min = 0.4\n",
    "\tAlpha_TP9_v = 0.8\n",
    "\tAlpha_TP10_v = 0.8\n",
    "\t\n",
    "\tBeta_AF7_min = 0.0\n",
    "\tBeta_AF8_min = 0.0\n",
    "\tBeta_AF7_v = 1.4\n",
    "\tBeta_AF8_v = 1.4\n",
    "\t\n",
    "\tBeta_TP9_min = 0.0\n",
    "\tBeta_TP10_min = 0.0\n",
    "\tBeta_TP9_v = 0.5\n",
    "\tBeta_TP10_v = 0.5\n",
    "\t\n",
    "\tGamma_AF7_min = 0.0\n",
    "\tGamma_AF8_min = 0.0\n",
    "\tGamma_AF7_v = 1.8\n",
    "\tGamma_AF8_v = 1.8\n",
    "\n",
    "\tGamma_TP9_min = 0.0\n",
    "\tGamma_TP10_min = 0.0\n",
    "\tGamma_TP9_v = 0.4\n",
    "\tGamma_TP10_v = 0.4\n",
    "\t\n",
    "\tfor p in range(100):\n",
    "\t\tinputs[p, Delta_TP9_i] = Delta_TP9_v if p % 2 == 0 else Delta_TP9_min\n",
    "\t\tinputs[p, Delta_AF7_i] = Delta_AF7_v if p % 2 == 0 else Delta_AF7_min\n",
    "\t\tinputs[p, Delta_AF8_i] = Delta_AF8_v if p % 2 == 0 else Delta_AF8_min\n",
    "\t\tinputs[p, Delta_TP10_i] = Delta_TP10_v if p % 2 == 0 else Delta_TP10_min\n",
    "\t\tinputs[p, Theta_TP9_i] = Theta_TP9_v if p % 2 == 0 else Theta_TP9_min\n",
    "\t\tinputs[p, Theta_AF7_i] = Theta_AF7_v if p % 2 == 0 else Theta_AF7_min\n",
    "\t\tinputs[p, Theta_AF8_i] = Theta_AF8_v if p % 2 == 0 else Theta_AF8_min\n",
    "\t\tinputs[p, Theta_TP10_i] = Theta_TP10_v if p % 2 == 0 else Theta_TP10_min\n",
    "\t\tinputs[p, Alpha_TP9_i] = Alpha_TP9_v if p % 2 == 0 else Alpha_TP9_min\n",
    "\t\tinputs[p, Alpha_AF7_i] = Alpha_AF7_v if p % 2 == 0 else Alpha_AF7_min\n",
    "\t\tinputs[p, Alpha_AF8_i] = Alpha_AF8_v if p % 2 == 0 else Alpha_AF8_min\n",
    "\t\tinputs[p, Alpha_TP10_i] = Alpha_TP10_v if p % 2 == 0 else Alpha_TP10_min\n",
    "\n",
    "\t\tinputs[p, Beta_TP9_i] = Beta_TP9_v * p / 100 if Beta_TP9_v * p / 100 > Beta_TP9_min else Beta_TP9_min\n",
    "\t\tinputs[p, Beta_AF7_i] = Beta_AF7_v * p / 100 if Beta_AF7_v * p / 100 > Beta_AF7_min else Beta_AF7_min\n",
    "\t\tinputs[p, Beta_AF8_i] = Beta_AF8_v * p / 100 if Beta_AF8_v * p / 100 > Beta_AF8_min else Beta_AF8_min\n",
    "\t\tinputs[p, Beta_TP10_i] = Beta_TP10_v * p / 100 if Beta_AF8_v * p / 100 > Beta_AF8_min else Beta_AF8_min\n",
    "\t\tinputs[p, Gamma_TP9_i] = Gamma_TP9_v * p / 100 if Gamma_TP9_v * p / 100 > Gamma_TP9_min else Gamma_TP9_min\n",
    "\t\tinputs[p, Gamma_AF7_i] = Gamma_AF7_v * p / 100 if Gamma_AF7_v * p / 100 > Gamma_AF7_min else Gamma_AF7_min\n",
    "\t\tinputs[p, Gamma_AF8_i] = Gamma_AF8_v * p / 100 if Gamma_AF8_v * p / 100 > Gamma_AF8_min else Gamma_AF8_min\n",
    "\t\tinputs[p, Gamma_TP10_i] = Gamma_TP10_v * p / 100 if Gamma_TP10_v * p / 100 > Gamma_TP10_min else Gamma_TP10_min\n",
    "\n",
    "\t\tlabels[p] = p / 100\n",
    "\n",
    "\t#print()\t\n",
    "\n",
    "#['TimeStamp','Gyro_X','Gyro_Y','Gyro_Z','Accelerometer_X','Accelerometer_Y','Accelerometer_Z','AUX_RIGHT','HSI_TP9','HSI_AF7',\n",
    "#'HSI_AF8','HSI_TP10','RAW_TP9','RAW_AF7','RAW_AF8','RAW_TP10','HeadBandOn','Battery','Elements','Mellow','Concentration']\n",
    "\n",
    "filename = 'C:\\\\Users\\\\Stephen Vivash\\\\Documents\\\\Yijing\\\\Muse\\\\2024-07-29-17-24-00-Muse.csv'\n",
    "#filename = 'C:\\\\Users\\\\Stephen Vivash\\\\Documents\\\\Yijing\\\\Muse\\\\2024-08-10-18-07-51-Muse.csv'\n",
    "all_columns = ['Delta_TP9','Delta_AF7','Delta_AF8','Delta_TP10','Theta_TP9','Theta_AF7','Theta_AF8','Theta_TP10','Alpha_TP9','Alpha_AF7','Alpha_AF8','Alpha_TP10',\n",
    "\t\t'Beta_TP9','Beta_AF7','Beta_AF8','Beta_TP10','Gamma_TP9','Gamma_AF7','Gamma_AF8','Gamma_TP10']\n",
    "graph_columns = [   'Delta_TP9'   ,'Alpha_AF7','Alpha_AF8','Beta_AF7','Beta_AF8','Gamma_AF7','Gamma_AF8']\n",
    "#graph_columns = ['Alpha_TP9','Alpha_AF7','Alpha_AF8','Alpha_TP10','Beta_TP9','Beta_AF7','Beta_AF8','Beta_TP10','Gamma_TP9','Gamma_AF7','Gamma_AF8','Gamma_TP10']) \n",
    "tail = 11000\n",
    "\n",
    "def train_eeg_model1():\n",
    "\n",
    "\tinputs = torch.zeros(100, 20, dtype=torch.float32, requires_grad=False).to('cpu')\n",
    "\tlabels = torch.zeros(100, 1, dtype=torch.float32, requires_grad=False).to('cpu')\n",
    "\tGenerateData(inputs, labels)\n",
    "\t#df = pd.DataFrame(inputs.numpy(), columns=all_columns)\n",
    "\t#df.plot(kind='line',legend=True,figsize=(20, 10))\n",
    "\t#print(inputs)\n",
    "\t#print(labels)\n",
    "\n",
    "\tdata = pd.read_csv(filename, usecols=all_columns) \n",
    "\tdata = data.tail(tail)\n",
    "\tdata = data.head(100)\n",
    "\tdata.plot(kind='line',legend=True,figsize=(20, 10))\n",
    "\n",
    "\t# Convert NumPy array to PyTorch tensor\n",
    "\t#inputs = torch.from_numpy(data.to_numpy()).float()\n",
    "\t#print(inputs)\n",
    "\n",
    "\t# Normalize the input data\n",
    "\tmean = inputs.mean(dim=0, keepdim=True)\n",
    "\t#print('mean', mean)\n",
    "\tstd = inputs.std(dim=0, keepdim=True)\n",
    "\t#print('std', std)\n",
    "\t#inputs = (inputs - mean) / std\n",
    "\t#print('inputs', inputs)\n",
    "\tprint()\n",
    "\n",
    "\tmodel = EegNN().to('cpu')\n",
    "\n",
    "\t# Define a loss function and an optimizer\n",
    "\tcriterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
    "\toptimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent optimizer  0.001\n",
    "\n",
    "\t# Training loop\n",
    "\tnum_epochs = 100_000 #50_000 # 1_000_000\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\t# Zero the gradients\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\t# Forward pass: compute the model output\n",
    "\t\toutputs = model(inputs)\n",
    "\t\t# Compute the loss\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\t# Backward pass: compute gradients\n",
    "\t\tloss.backward()\n",
    "\t\t# Update weights\n",
    "\t\toptimizer.step()\n",
    "\t\t# Print loss every 100 epochs\n",
    "\t\tif (epoch+1) % 10_000 == 0: # 100_000 == 0:\n",
    "\t\t\tprint(f'Epoch [{epoch+1}/{num_epochs}], Cost: {loss.item():.4f}')\n",
    "\n",
    "\tprint()\n",
    "\n",
    "\twith open('eeg_model1_state.pt', 'wb') as f: \n",
    "\t\tsave(model.state_dict(), f) \n",
    "\n",
    "def test_eeg_model1():\n",
    "\tdf1 = pd.read_csv(filename,\tusecols=all_columns) \n",
    "\tdf1 = df1.tail(tail)\n",
    "\tdf1 = df1.head(100)\n",
    "\n",
    "\tinputs = torch.zeros(100, 20, dtype=torch.float32, requires_grad=False).to('cpu')\n",
    "\tlabels = torch.zeros(100, 1, dtype=torch.float32, requires_grad=False).to('cpu')\n",
    "\tGenerateData(inputs, labels)\n",
    "\t\n",
    "\tmodel = EegNN().to('cpu')\n",
    "\n",
    "\t# Convert NumPy array to PyTorch tensor\n",
    "\tinputs = torch.from_numpy(df1.to_numpy()).float().to('cpu')\n",
    "\t#print(inputs)\n",
    "\n",
    "\t# Normalize the input data\n",
    "\t#mean = inputs.mean(dim=0, keepdim=True)\n",
    "\t#print('mean', mean)\n",
    "\t#std = inputs.std(dim=0, keepdim=True)\n",
    "\t#print('std', std)\n",
    "\t#inputs = (inputs - mean) / std\n",
    "\t#print('inputs', inputs)\n",
    "\t#print()\n",
    "\n",
    "\twith open('eeg_model1_state.pt', 'rb') as f: \n",
    "\t\tmodel.load_state_dict(load(f, weights_only=False))  \n",
    "\n",
    "\t#df2 = pd.DataFrame(inputs.numpy(), columns=all_columns)\n",
    "\tdf2 = pd.read_csv(filename,usecols=graph_columns) \n",
    "\tdf2 = df2.tail(tail)\n",
    "\tdf2 = df2.head(100)\n",
    "\tn2 = df2.to_numpy()\n",
    "\tdf2.columns.values[0] = 'Prediction'\n",
    "\n",
    "\tfor i in range(100):\n",
    "\t\te1 = inputs[i]\n",
    "\t\t#e1 = (e1 - mean) / std\n",
    "\t\tp1 = model(e1)\n",
    "\t\tn2[i][0] = p1.item()\n",
    "\t\t#print(f'Prediction: {p1.item():.2f}')\n",
    "\n",
    "\t#df2.to_csv('eeg_model1.csv', index=False)\n",
    "\n",
    "\tdf2.plot(kind='line',legend=True,figsize=(20, 10), linewidth=3.0)\n",
    "\n",
    "\tprint()\n",
    "\n",
    "#train_eeg_model1()\n",
    "test_eeg_model1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
